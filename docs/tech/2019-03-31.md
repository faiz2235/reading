# 企业 IT 架构转型之道：阿里巴巴中台战略思想与架构实战
这本书介绍了阿里巴巴电商系统架构的演变历史，并且对集团内一系列在工程上的实践进行了系统的总结。读完这本书，感觉不少知识点还是不太清楚，因为还没有过实战经验。但我相信，只要自己在以后的工作中多加实践，勤于思考和总结，一定会有更加深刻的认识。

以下是我对书中部分片段的摘录。

## 系统如何建设？
**最大的浪费不是重复建设，而是不断重复建设**。在早期往往一个新业务的上线除了数据可重复使用之外，服务却不能被重复使用。其实服务的重用会比数据重用带来更多好处。数据只是原始生产资源，服务则包含逻辑，是工厂的加工车间，如果加工过程也一样可以复制，将带来生产效率的大幅度提升。

系统的建设要**从生产型模型升级到运营型模型**，**从版本模型升级到迭代模型**，运营模型最大的优势是所有的积累都被沉淀，而生产模型会因为 10% 的差异而重新建设 100% 的系统。

## “烟囱式”系统建设模式
2008 年淘宝的技术团队同时支持着淘宝和天猫两大电商平台，而 1999 年成立的 B2B 电商平台 1688 一直拥有着自己的技术团队。三套电商体系的架构**完全独立**，各自应用**独立开发和运维**。可能是开发团队考虑到电商模式的不同，所以需要独立建设，或者新的业务团队认为在之前电商平台基础上改造成新模式，会有太多的技术和业务的历史包袱，从而促成了当时三座“烟囱”分别矗立支撑着当时阿里巴巴集团最为核心的电商业务。

![1688-taobao-tmall](/images/1688-taobao-tmall.png)

这种模式有三大**弊端**：

1. 重复功能建设和维护带来的重复投资。
1. 打通“烟囱式”系统间交互的集成和协作成本高昂。 
1. 不利于业务的沉淀和持续发展。

很多企业通过 ESB 系统实现多个独立系统的打通，这时候要求各个系统进行服务的改造和封装，多数情况下，服务封装的工作跟运维人员自身 KPI 考核没多大关系。所以现实中会出现：

1. 服务提供者团队不管是从 KPI 考核的角度，还是从认知上都会认为服务封装的任务已经完成，当他们收到新的服务需求时，心理是拒绝的，会出于多一事不如少一事的心态。
1. 服务提供者团队拥有不错的做事态度，也愿意改造服务以满足新业务需求，但受限于之前服务设计的通用性和业务前瞻性的不足，需要对之前业务逻辑做较大的改造。更多团队选择放弃对新业务需求的支持。

## 服务重用
今天的阿里巴巴已经将集团 20 多个核心业务中公共的、通用的业务以服务的方式沉淀到了**共享业务事业部**，共享业务事业部在中台战略中扮演着至关重要的作用，整个集团的核心业务能力均建立在这样一套共享服务体系之上，使得我们在今天的业务支持中，真正发挥出了 SOA 架构的核心价值——**服务重用**。

## 服务需要不断的业务滋养
服务最不需要“业务稳定”。一个服务如果一味追求功能的不变，一定程度上就是固步自封，这样的做法是在逼着其它系统去建同样的“轮子”，当越来越多的系统都采用自建“轮子”的方式满足自身系统对这部分业务的需求时，之前的这个服务就少有人问津，逐渐退出历史舞台。

服务需要不停的滋养，只有在滋养中才能从最初仅供单薄业务功能的服务逐渐成长为企业最为宝贵的 IT 资产，而服务所需的滋养正是**新的业务不断进行服务的接入**。

## 赋予业务快速创新和试错能力
企业要想在互联网时代相比同行业的竞争对手们真正产生差异化的竞争力，**业务试错**是一个非常重要的能力，只有先人一步，唯快不破，才能帮助企业抢占商业先级的制高点。互联网时代的竞争只有第一，没有第二。

如果有一个好的业务想法，需要投入 20 个人、 4 个月时间，结果还可能建设的系统达不到预期市场，那么这个业务试错的成本是高昂的，任何一个企业都很难支持这样的试错。如果企业**打造了很好的业务中台**，可以让 3 个人基于中台提供的核心服务在 2 周内建成系统并推向市场，看看市场的反馈来决定是否加大对这个新业务的投入，那么任何一家企业的领导都会乐意做这样的尝试。

> 美军在二战时，以军为单位作战；到了越战时，以营为单位作战；到了中东战争时，以 7/11 人的极小班排去作战。它是今天全世界范围内最灵活的军事组织，也是核心竞争力和打击能力最强的组织。美军之所以能够灵活作战，敢放这么小的团队到前面，是因为有非常强的导弹指挥系统，有非常强大的中后台能力，能支持这样的小团队快速做判断，并且引领整个进攻完成。

这与阿里巴巴如今的“大中台、小前端”战略完全一致，与华为公司提的头狼团队也有异曲同工之妙。

小的前端团队具有以下特征：

1. 团队协同效率最高。
1. 对战机（商机）的把握更加敏锐。
1. 调整方向更加快捷。
1. 一旦发现正确目标，全力投入扩大战果。

一旦前端的作战团队找到了正确的攻击目标，接下来一个远程呼唤，后端的中台炮火群会瞬间摧毁目标，这就是中台阵型发挥威力的最佳体现。如今的互联网公司都在着力打造自己的业务中台，通过中台资源的优势，吸引那些有能力基于这些中台进行更好业务创新的人才加入，形成了**公司搭舞台、各路英豪在这个舞台上展现自己才能**的格局，这应该是接下来一段时间顶尖公司与顶尖人才间建立合作的最佳方式。

## 淘宝平台“服务化”
2007 年，淘宝拥有超过 500 人的技术团队，整个淘宝网站是一个几百兆字节的 WAR 包，大小功能模块超过 200 个。当时淘宝业务计划处于每几个月就翻倍的高速发展期，这样的应用架构给淘宝技术团队带来了以下几个主要问题：

1. **项目团队间协同成本高，业务响应慢**。

    代码合并时，各种 jar 包冲突、代码不一致，不同团队间需要进行各种确认和协调工作。

1. **应用复杂度已超出人的认知负载**。

    越来越复杂的淘宝平台，各种业务错综复杂揉在一起，没有谁能完全清楚每一个功能和业务流程的细节，因为人的认知负载是有限的。这样造成了每一次淘宝平台整体打包发布时，蕴含着非常大的风险，小小的功能改动可能会给其它功能带来未知的风险，整个平台给人一种“牵一发而动全身”的感觉。

1. **错误难以隔离**。

    一些非核心功能的设计不合理、代码质量差可能引起整个平台的业务受到全面影响，根本原因就是核心功能和非核心功能的代码都运行在同一个环境中，任何一个小的问题都可能造成应用实例的崩溃，从而影响到整个淘宝平台的正常运行。

1. **数据库连接能力很难扩展**。

    所有业务功能在一个 WAR 包内，所有的数据也保存在同一数据库集群中，而**数据库集群的数据库连接数量是有上限的**，这就造成数据库连接数量的资源随着应用实例数量的增加而越来越捉襟见肘。

1. **应用扩展成本高**。

    系统出现业务处理瓶颈时，可能只是某一个或几个功能模块负载较高造成的，但系统所有的功能都打包在一起，无法对单独的几个功能模块进行服务能力的扩展，只能将整个完整的应用进行扩容，带来了资源额外配置的消耗。


解决以上问题的根本就在于**业务的拆分**。

淘宝从现有应用中选择了**用户相关的功能点**作为试点，剥离出了**用户服务中心**，主要出发点在于用户的业务逻辑相对独立和简单，而且服务功能的复用率最高。改造后，很多问题得到了解决：

1. 降低了不同模块开发团队间的协同成本，业务响应更迅捷。
1. 大大降低系统间的耦合度以及整体复杂度，各个开发团队可专注于各自的业务模块。
1. 避免个别模块的错误给整体带来的影响。
1. 业务拆分后解放了对单数据库集群连接数的能力依赖。
1. 做到针对性的业务能力扩容，减少不必要的资源浪费。

## “中心化”与“去中心化”服务框架对比
SOA 的主要特性：

- 面向服务的分布式计算
- 服务间松散耦合
- 支持服务的组装
- 服务注册和自动发现
- 以服务契约方式定义服务交互方式

目前，互联网有两套架构，一套是以 ESB（企业服务总线），以中心化方式实现 SOA；一套是“去中心化”架构。

### ESB 模式的“中心化”服务架构
系统间交互时，通过在 ESB 上进行一次调整，避免了因为服务提供者接口的变化而导致需要调用此服务的服务调用者都进行修改的现象，实现了对服务接口变化带来影响的隔离。ESB 架构降低了系统间的耦合，更方便、高效地实现了对新系统的集成，同时也在服务负载均衡、服务管控等方面提供了相对“点对点”模式更专业的能力。

在 ESB 这样一个**中心服务总线**上，提供了诸如对各种技术接口的适配接入、数据格式转换、数据裁剪、服务请求路由等功能。核心目的是让企业客户能基于这些 SOA 的产品实现系统间的互联互通，同时提高开发集成效率，更快地实现 SOA 项目的落地。

### “去中心化”分布式服务架构解决的问题
“去中心化”服务架构除了对于 SOA 特性的实现和满足外，相比“中心化”服务架构最重要的不同就是服务提供者和服务调用者之间在进行服务交互时**无需通过任何服务路由中介**，避免因为“中心点”带来平台能力难扩展问题，以及潜在的“雪崩”影响。

### “中心化”服务框架为什么不适合互联网场景
1. **服务调用方式的不同带来业务的响应和扩展成本**

    传统 ESB，每一次服务的调用者要向服务提供者进行服务交互请求时都必须通过中心的 ESB 来进行路由。

    ![consumer-ESB-provider](/images/consumer-ESB-provider.png)

    经过服务总线路由过的服务交互，共出现 4 次网络会话创建和数据传输，而“去中心化”服务架构中服务的交互，一次服务的调用只有两次网络会话创建和数据传输，开销少了一半。

    ![consumer-provider](/images/consumer-provider.png)

    从逻辑上看，所有服务调用都通过服务总线，服务总线的访问和计算压力都非常大，虽然也可通过集群部署的方式进行压力的分担，但一般企业服务总线包含的功能非常多，使得企业服务总线一般对服务器的要求都比较高，每次扩容升级，都需要不少投入。

1. **“雪崩”效应束缚了“中心化”服务框架的扩展能力**

    基于企业服务总线构建的服务体系，会成为企业服务调度的核心枢纽。如果在 ESB 集群有 10 台实例，服务访问的峰值，每个企业服务总线负载达 80%，而此时某个应用对某服务产生了不规范的服务调用，或者出现其它异常故障，导致 10 台企业服务总线的实例有 1 台出现了问题，此时压力落在剩下的 9 台 ESB 服务器上，每台服务器负载水位超过 88%，集群可能在瞬间被访问洪流冲垮，这就是典型的雪崩效应。

    一旦遇上雪崩，故障恢复的时间和成本非常高昂。一台一台重启服务器已经不能进行故障的恢复，因为一旦服务启动起来，前端的访问洪流会立即再次压垮启动后的服务器。唯一正确的方式则是首先切断前端应用对企业服务总线的服务请求，让这 10 台服务器全部启动后，再开放服务请求，这样才能恢复系统的运行。但这样没定位问题所在，而着急恢复系统，这样的系统运行其实处于一个“脆弱”的状态，之前造成服务实例宕机的问题可能让“雪崩”事故再次上演。